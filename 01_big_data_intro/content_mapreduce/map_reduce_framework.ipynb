{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Framework\n",
    " \n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES\n",
    "\n",
    "Hadoop: The Definitive Guide, Tom White\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to the MapReduce framework\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- MapReduce\n",
    "- Jobs\n",
    "- Tasks\n",
    "- Map operation\n",
    "- Reduce operation\n",
    "- Shuffle operation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Introducing MapReduce**\n",
    "\n",
    "`MapReduce` is a programming framework for processing large data sets with a parallel, distributed algorithm on a cluster.  \n",
    "\n",
    "First the data set is “mapped” into a collection of `<key, value>` pairs, and then “reduced” over all pairs with the same key.  \n",
    "We need to discuss what this means.  \n",
    "\n",
    "The operations are surprisingly broad, enabling them to handle a wide range of use cases.\n",
    "\n",
    "The popular minimal example is to do a word count on some text.\n",
    "\n",
    "Initially published in 2004 by employees at Google\n",
    "\n",
    "A `MapReduce` job is a unit of work the client wants to be performed  \n",
    "\n",
    "A job consists of:  \n",
    "\n",
    "- Input data\n",
    "- `MapReduce` program\n",
    "- configs\n",
    "\n",
    "The job will be divided into *tasks*  \n",
    "Two types of tasks: *map tasks* and *reduce tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on `Hadoop`**  \n",
    "\n",
    "`MapReduce` forms the computation paradigm for Hadoop  \n",
    "\n",
    "`Hadoop` was created by Doug Cutting and Mike Cafarella.  Version 1 was called Nutch.  \n",
    "\n",
    "Yahoo! was instrumental in providing a dedicated team and resources to turn Hadoop into a system that ran at web scale.\n",
    "\n",
    "Hadoop is developed and maintained by the Apache Software Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on Hadoop and Spark**  \n",
    "\n",
    "Spark does not follow the `MapReduce` framework in the same way that `Hadoop` does, where `Mapper` and `Reducer` functions are detailed explicitly.  This is actually a big advantage of Spark.\n",
    "\n",
    "Spark and Hadoop can be better together.  For example, some teams will use Hadoop data storage (HDFS) with Spark.  \n",
    "That is, a Spark job can read data from HDFS, and write results to HDFS.\n",
    "\n",
    "Reported runtimes:\n",
    "Spark can be as much as 10 times faster than `MapReduce` for batch processing, and up to 100 times faster for in-memory analytics.\n",
    "\n",
    "A large fraction of this course will focus on Spark for various tasks.\n",
    "\n",
    "Next we illustrate the `MapReduce` framework with an example.\n",
    "\n",
    "**MapReduce Word Count Process Diagram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"map_reduce_example2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Things to Notice:**\n",
    "\n",
    "**Splitting Step**  \n",
    "A given number of records is assigned to each worker\n",
    "\n",
    "NOTE:  \n",
    "More splits means less processing time per split  \n",
    "However, the overhead of managing splits and `map` task creation starts to dominate total job execution time  \n",
    "\n",
    "**Mapping Step**  \n",
    "Each token (word) is given a value of 1 representing the count.  \n",
    "The token, count forms a `<key, value> pair.`  \n",
    "If the same pair appears $n$ times on a worker, there would be $n$ entries of `<map, 1>`.  In other words, no aggregation is done at this step (it happens at the `reduce` step).  \n",
    "\n",
    "**Shuffling Step**  \n",
    "The `<key, value>` pairs are moved across machines at this point, so that pairs with the same key are gathered on the same machine.  This is a costly step in the process.\n",
    "\n",
    "**Reducing Step**  \n",
    "A `reducer` applies a given operation over the values by key.  For this word count example, the counts would be summed for each key and stored with the key.\n",
    "\n",
    "**Map and Reduce in General**  \n",
    "\n",
    "**Map**  \n",
    "Higher-order function that applies a function to each element of a list (e.g., $x => x**2$)\n",
    "\n",
    "**Reduce**  \n",
    "Higher-order function that applies a combining operation (e.g,. cumulative sum of values)\n",
    "\n",
    "**Combiner Functions**  \n",
    "It can be expensive to shuffle data within the cluster, and the limiting factor is the bandwidth on the cluster.\n",
    "\n",
    "`Combiner Functions` can be run on the map output, before the shuffle occurs. This acts locally on each node, and reduces some of the data transfer.  Note this won’t always work (e.g., can’t be used for computing the average, but CAN be used for computing the maximum).\n",
    "\n",
    "\n",
    "**Word Count Example**  \n",
    "Data source: Spark README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 12:49:27 WARN Utils: Your hostname, Eileanors-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.0.110 instead (on interface en0)\n",
      "24/09/04 12:49:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/04 12:49:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# context manager: minimal settings\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.110:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x105d0d790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uva_seal.png',\n",
       " 'map_reduce_framework.ipynb',\n",
       " 'map_reduce_example2.png',\n",
       " 'README.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list files in the directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/elarocco/Desktop/uva_phd_2024/distributed_computing/development/distributed_computing/01_big_data_intro/content_mapreduce'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "lines = sc.textFile(\"README.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=35826) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.',\n",
       " '',\n",
       " 'https://spark.apache.org/',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some records\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show top 10 most frequent words** (we will examine in detail later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13, 'the'),\n",
       " (11, 'Spark'),\n",
       " (7, 'to'),\n",
       " (6, 'for'),\n",
       " (6, 'and'),\n",
       " (6, 'a'),\n",
       " (6, 'can'),\n",
       " (6, 'run'),\n",
       " (5, 'using'),\n",
       " (5, 'is')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda x: x.split())\n",
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How are `map()` and `flatMap()` different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribute data to the workers\n",
    "data = sc.parallelize([3,4,5])\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map: a list of lists  \n",
    "flatMap: a flattened list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Copy the word count code into the cell below.  Modify it to print the top 20 wordcount pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13, 'the'),\n",
       " (11, 'Spark'),\n",
       " (7, 'to'),\n",
       " (6, 'for'),\n",
       " (6, 'and'),\n",
       " (6, 'a'),\n",
       " (6, 'can'),\n",
       " (6, 'run'),\n",
       " (5, 'using'),\n",
       " (5, 'is'),\n",
       " (4, 'with'),\n",
       " (4, '*'),\n",
       " (4, 'examples'),\n",
       " (4, 'in'),\n",
       " (4, 'also'),\n",
       " (4, 'You'),\n",
       " (3, 'an'),\n",
       " (3, 'you'),\n",
       " (3, 'use'),\n",
       " (3, 'including')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)\n",
    "wordcounts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Copy the word count code into the cell below.  Modify it to filter out rows containing the word *Spark* and then execute the word count, returning the top 10 wordcount pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 'the'),\n",
       " (6, 'to'),\n",
       " (5, 'run'),\n",
       " (4, '*'),\n",
       " (4, 'can'),\n",
       " (3, 'you'),\n",
       " (3, 'examples'),\n",
       " (3, 'if'),\n",
       " (3, 'a'),\n",
       " (2, 'command,')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = lines.filter(lambda x: \"Spark\" not in x) \\\n",
    "                  .flatMap(lambda x: x.split()) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "wordcounts = lines.flatMap(lambda x: x.split()) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(lambda x,y:x+y) \\\n",
    "            .map(lambda x:(x[1],x[0])) \\\n",
    "            .sortByKey(False)\n",
    "\n",
    "wordcounts.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7, 'the'),\n",
       " (6, 'to'),\n",
       " (5, 'run'),\n",
       " (4, '*'),\n",
       " (4, 'can'),\n",
       " (3, 'you'),\n",
       " (3, 'examples'),\n",
       " (3, 'if'),\n",
       " (3, 'a'),\n",
       " (2, 'command,')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2)\n",
    "\n",
    "wordcounts = lines.filter(lambda x: \"Spark\" not in x) \\\n",
    "            .flatMap(lambda x: x.split()) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(lambda x,y:x+y) \\\n",
    "            .map(lambda x:(x[1],x[0])) \\\n",
    "            .sortByKey(False)\n",
    "\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
