{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 10:23:54 WARN Utils: Your hostname, Eileanors-Laptop.local resolves to a loopback address: 127.0.0.1; using 172.25.161.99 instead (on interface en0)\n",
      "24/09/26 10:23:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/26 10:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 10:24:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|      id|diagnosis|   f1|   f2|   f3|    f4|     f5|     f6|    f7|     f8|    f9|    f10|   f11|   f12|  f13|  f14|     f15|    f16|    f17|    f18|    f19|     f20|  f21|  f22|  f23|   f24|   f25|   f26|   f27|   f28|   f29|    f30|\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|  842302|        M|17.99|10.38|122.8|1001.0| 0.1184| 0.2776|0.3001| 0.1471|0.2419|0.07871| 1.095|0.9053|8.589|153.4|0.006399|0.04904|0.05373|0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119|0.2654|0.4601| 0.1189|\n",
      "|  842517|        M|20.57|17.77|132.9|1326.0|0.08474|0.07864|0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225|0.01308| 0.0186| 0.0134|0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416| 0.186| 0.275|0.08902|\n",
      "|84300903|        M|19.69|21.25|130.0|1203.0| 0.1096| 0.1599|0.1974| 0.1279|0.2069|0.05999|0.7456|0.7869|4.585|94.03| 0.00615|0.04006|0.03832|0.02058| 0.0225|0.004571|23.57|25.53|152.5|1709.0|0.1444|0.4245|0.4504| 0.243|0.3613|0.08758|\n",
      "|84348301|        M|11.42|20.38|77.58| 386.1| 0.1425| 0.2839|0.2414| 0.1052|0.2597|0.09744|0.4956| 1.156|3.445|27.23| 0.00911|0.07458|0.05661|0.01867|0.05963|0.009208|14.91| 26.5|98.87| 567.7|0.2098|0.8663|0.6869|0.2575|0.6638|  0.173|\n",
      "|84358402|        M|20.29|14.34|135.1|1297.0| 0.1003| 0.1328| 0.198| 0.1043|0.1809|0.05883|0.7572|0.7813|5.438|94.44| 0.01149|0.02461|0.05688|0.01885|0.01756|0.005115|22.54|16.67|152.2|1575.0|0.1374| 0.205|   0.4|0.1625|0.2364|0.07678|\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into dataframe\n",
    "df = spark.read.csv(DATA_FILEPATH,  inferSchema=True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(diagnosis='B'), Row(diagnosis='M')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('diagnosis').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+------+-----+-----+--------+--------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+----------------+\n",
      "|      id|diagnosis|   f1|   f2|   f3|    f4|     f5|     f6|     f7|     f8|    f9|    f10|   f11|   f12|  f13|  f14|     f15|     f16|    f17|     f18|    f19|     f20|  f21|  f22|  f23|   f24|   f25|   f26|   f27|    f28|   f29|    f30|diagnosisEncoded|\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+------+-----+-----+--------+--------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+----------------+\n",
      "|  842302|        M|17.99|10.38|122.8|1001.0| 0.1184| 0.2776| 0.3001| 0.1471|0.2419|0.07871| 1.095|0.9053|8.589|153.4|0.006399| 0.04904|0.05373| 0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119| 0.2654|0.4601| 0.1189|             1.0|\n",
      "|  842517|        M|20.57|17.77|132.9|1326.0|0.08474|0.07864| 0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225| 0.01308| 0.0186|  0.0134|0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416|  0.186| 0.275|0.08902|             1.0|\n",
      "|84300903|        M|19.69|21.25|130.0|1203.0| 0.1096| 0.1599| 0.1974| 0.1279|0.2069|0.05999|0.7456|0.7869|4.585|94.03| 0.00615| 0.04006|0.03832| 0.02058| 0.0225|0.004571|23.57|25.53|152.5|1709.0|0.1444|0.4245|0.4504|  0.243|0.3613|0.08758|             1.0|\n",
      "|84348301|        M|11.42|20.38|77.58| 386.1| 0.1425| 0.2839| 0.2414| 0.1052|0.2597|0.09744|0.4956| 1.156|3.445|27.23| 0.00911| 0.07458|0.05661| 0.01867|0.05963|0.009208|14.91| 26.5|98.87| 567.7|0.2098|0.8663|0.6869| 0.2575|0.6638|  0.173|             1.0|\n",
      "|84358402|        M|20.29|14.34|135.1|1297.0| 0.1003| 0.1328|  0.198| 0.1043|0.1809|0.05883|0.7572|0.7813|5.438|94.44| 0.01149| 0.02461|0.05688| 0.01885|0.01756|0.005115|22.54|16.67|152.2|1575.0|0.1374| 0.205|   0.4| 0.1625|0.2364|0.07678|             1.0|\n",
      "|  843786|        M|12.45| 15.7|82.57| 477.1| 0.1278|   0.17| 0.1578|0.08089|0.2087|0.07613|0.3345|0.8902|2.217|27.19| 0.00751| 0.03345|0.03672| 0.01137|0.02165|0.005082|15.47|23.75|103.4| 741.6|0.1791|0.5249|0.5355| 0.1741|0.3985| 0.1244|             1.0|\n",
      "|  844359|        M|18.25|19.98|119.6|1040.0|0.09463|  0.109| 0.1127|  0.074|0.1794|0.05742|0.4467|0.7732| 3.18|53.91|0.004314| 0.01382|0.02254| 0.01039|0.01369|0.002179|22.88|27.66|153.2|1606.0|0.1442|0.2576|0.3784| 0.1932|0.3063|0.08368|             1.0|\n",
      "|84458202|        M|13.71|20.83| 90.2| 577.9| 0.1189| 0.1645|0.09366|0.05985|0.2196|0.07451|0.5835| 1.377|3.856|50.96|0.008805| 0.03029|0.02488| 0.01448|0.01486|0.005412|17.06|28.14|110.6| 897.0|0.1654|0.3682|0.2678| 0.1556|0.3196| 0.1151|             1.0|\n",
      "|  844981|        M| 13.0|21.82| 87.5| 519.8| 0.1273| 0.1932| 0.1859|0.09353| 0.235|0.07389|0.3063| 1.002|2.406|24.32|0.005731| 0.03502|0.03553| 0.01226|0.02143|0.003749|15.49|30.73|106.2| 739.3|0.1703|0.5401| 0.539|  0.206|0.4378| 0.1072|             1.0|\n",
      "|84501001|        M|12.46|24.04|83.97| 475.9| 0.1186| 0.2396| 0.2273|0.08543| 0.203|0.08243|0.2976| 1.599|2.039|23.94|0.007149| 0.07217|0.07743| 0.01432|0.01789| 0.01008|15.09|40.68|97.65| 711.4|0.1853| 1.058| 1.105|  0.221|0.4366| 0.2075|             1.0|\n",
      "|  845636|        M|16.02|23.24|102.7| 797.8|0.08206|0.06669|0.03299|0.03323|0.1528|0.05697|0.3795| 1.187|2.466|40.51|0.004029|0.009269|0.01101|0.007591| 0.0146|0.003042|19.19|33.88|123.8|1150.0|0.1181|0.1551|0.1459|0.09975|0.2948|0.08452|             1.0|\n",
      "|84610002|        M|15.78|17.89|103.6| 781.0| 0.0971| 0.1292|0.09954|0.06606|0.1842|0.06082|0.5058|0.9849|3.564|54.16|0.005771| 0.04061|0.02791| 0.01282|0.02008|0.004144|20.42|27.28|136.5|1299.0|0.1396|0.5609|0.3965|  0.181|0.3792| 0.1048|             1.0|\n",
      "|  846226|        M|19.17| 24.8|132.4|1123.0| 0.0974| 0.2458| 0.2065| 0.1118|0.2397|  0.078|0.9555| 3.568|11.07|116.2|0.003139| 0.08297| 0.0889|  0.0409|0.04484| 0.01284|20.96|29.94|151.7|1332.0|0.1037|0.3903|0.3639| 0.1767|0.3176| 0.1023|             1.0|\n",
      "|  846381|        M|15.85|23.95|103.7| 782.7|0.08401| 0.1002|0.09938|0.05364|0.1847|0.05338|0.4033| 1.078|2.903|36.58|0.009769| 0.03126|0.05051| 0.01992|0.02981|0.003002|16.84|27.66|112.0| 876.5|0.1131|0.1924|0.2322| 0.1119|0.2809|0.06287|             1.0|\n",
      "|84667401|        M|13.73|22.61| 93.6| 578.3| 0.1131| 0.2293| 0.2128|0.08025|0.2069|0.07682|0.2121| 1.169|2.061|19.21|0.006429| 0.05936|0.05501| 0.01628|0.01961|0.008093|15.03|32.01|108.8| 697.7|0.1651|0.7725|0.6943| 0.2208|0.3596| 0.1431|             1.0|\n",
      "|84799002|        M|14.54|27.54|96.73| 658.8| 0.1139| 0.1595| 0.1639|0.07364|0.2303|0.07077|  0.37| 1.033|2.879|32.55|0.005607|  0.0424|0.04741|  0.0109|0.01857|0.005466|17.46|37.13|124.1| 943.2|0.1678|0.6577|0.7026| 0.1712|0.4218| 0.1341|             1.0|\n",
      "|  848406|        M|14.68|20.13|94.74| 684.5|0.09867|  0.072|0.07395|0.05259|0.1586|0.05922|0.4727|  1.24|3.195| 45.4|0.005718| 0.01162|0.01998| 0.01109| 0.0141|0.002085|19.07|30.88|123.4|1138.0|0.1464|0.1871|0.2914| 0.1609|0.3029|0.08216|             1.0|\n",
      "|84862001|        M|16.13|20.68|108.1| 798.8|  0.117| 0.2022| 0.1722| 0.1028|0.2164|0.07356|0.5692| 1.073|3.854|54.18|0.007026| 0.02501|0.03188| 0.01297|0.01689|0.004142|20.96|31.48|136.8|1315.0|0.1789|0.4233|0.4784| 0.2073|0.3706| 0.1142|             1.0|\n",
      "|  849014|        M|19.81|22.15|130.0|1260.0|0.09831| 0.1027| 0.1479|0.09498|0.1582|0.05395|0.7582| 1.017|5.865|112.4|0.006494| 0.01893|0.03391| 0.01521|0.01356|0.001997|27.32|30.88|186.8|2398.0|0.1512| 0.315|0.5372| 0.2388|0.2768|0.07615|             1.0|\n",
      "| 8510426|        B|13.54|14.36|87.46| 566.3|0.09779|0.08129|0.06664|0.04781|0.1885|0.05766|0.2699|0.7886|2.058|23.56|0.008462|  0.0146|0.02387| 0.01315| 0.0198|  0.0023|15.11|19.26| 99.7| 711.2| 0.144|0.1773| 0.239| 0.1288|0.2977|0.07259|             0.0|\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+------+-----+-----+--------+--------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"diagnosisEncoded\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+----------------+-------------+\n",
      "|id      |diagnosis|f1   |f2   |f3   |f4    |f5     |f6     |f7    |f8     |f9    |f10    |f11   |f12   |f13  |f14  |f15     |f16    |f17    |f18    |f19    |f20     |f21  |f22  |f23  |f24   |f25   |f26   |f27   |f28   |f29   |f30    |diagnosisEncoded|features     |\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+----------------+-------------+\n",
      "|842302  |M        |17.99|10.38|122.8|1001.0|0.1184 |0.2776 |0.3001|0.1471 |0.2419|0.07871|1.095 |0.9053|8.589|153.4|0.006399|0.04904|0.05373|0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119|0.2654|0.4601|0.1189 |1.0             |[17.99,10.38]|\n",
      "|842517  |M        |20.57|17.77|132.9|1326.0|0.08474|0.07864|0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225|0.01308|0.0186 |0.0134 |0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416|0.186 |0.275 |0.08902|1.0             |[20.57,17.77]|\n",
      "|84300903|M        |19.69|21.25|130.0|1203.0|0.1096 |0.1599 |0.1974|0.1279 |0.2069|0.05999|0.7456|0.7869|4.585|94.03|0.00615 |0.04006|0.03832|0.02058|0.0225 |0.004571|23.57|25.53|152.5|1709.0|0.1444|0.4245|0.4504|0.243 |0.3613|0.08758|1.0             |[19.69,21.25]|\n",
      "|84348301|M        |11.42|20.38|77.58|386.1 |0.1425 |0.2839 |0.2414|0.1052 |0.2597|0.09744|0.4956|1.156 |3.445|27.23|0.00911 |0.07458|0.05661|0.01867|0.05963|0.009208|14.91|26.5 |98.87|567.7 |0.2098|0.8663|0.6869|0.2575|0.6638|0.173  |1.0             |[11.42,20.38]|\n",
      "|84358402|M        |20.29|14.34|135.1|1297.0|0.1003 |0.1328 |0.198 |0.1043 |0.1809|0.05883|0.7572|0.7813|5.438|94.44|0.01149 |0.02461|0.05688|0.01885|0.01756|0.005115|22.54|16.67|152.2|1575.0|0.1374|0.205 |0.4   |0.1625|0.2364|0.07678|1.0             |[20.29,14.34]|\n",
      "+--------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(indexed)\n",
    "tr.select(\"*\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = tr.randomSplit([0.6, 0.4], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+---------------------------------------+\n",
      "|diagnosisEncoded|features     |scaledFeatures                         |\n",
      "+----------------+-------------+---------------------------------------+\n",
      "|1.0             |[15.46,19.48]|[4.32528603980796,4.543962571206202]   |\n",
      "|0.0             |[12.89,13.12]|[3.6062701845488103,3.0604101095598235]|\n",
      "|0.0             |[14.96,19.1] |[4.185399686644701,4.455322644252488]  |\n",
      "|1.0             |[13.17,18.66]|[3.684606542320235,4.352686939358713]  |\n",
      "|0.0             |[12.18,17.84]|[3.4076315630569827,4.161411307511224] |\n",
      "+----------------+-------------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-------------+--------------------------------------+\n",
      "|diagnosisEncoded|features     |scaledFeatures                        |\n",
      "+----------------+-------------+--------------------------------------+\n",
      "|0.0             |[12.94,16.17]|[3.620258819865136,3.771862154846216] |\n",
      "|1.0             |[20.26,23.03]|[5.668195030175244,5.372046099326429] |\n",
      "|0.0             |[12.63,20.76]|[3.533529280903916,4.842539167260819] |\n",
      "|0.0             |[14.26,19.65]|[3.989558792216139,4.583617275369705] |\n",
      "|0.0             |[13.85,17.21]|[3.8748519826222667,4.014455639140592]|\n",
      "+----------------+-------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\") #withMean = True centers the data - not good if sparse data\n",
    "\n",
    "#Scale Training Data\n",
    "scalerTrain = scaler.fit(train)\n",
    "scaledTrain = scalerTrain.transform(train)\n",
    "\n",
    "#Scale Testing Data\n",
    "scalerTest = scaler.fit(test)\n",
    "scaledTest = scalerTrain.transform(test)\n",
    "\n",
    "scaledTrain.select(\"diagnosisEncoded\",\"features\",\"scaledFeatures\").show(5, truncate=False)\n",
    "scaledTest.select(\"diagnosisEncoded\",\"features\",\"scaledFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 10:24:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39066757761187426,0.0]\n",
      "Intercept: -2.0137943460354157\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='diagnosisEncoded',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrain)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+\n",
      "|probability                             |prediction|\n",
      "+----------------------------------------+----------+\n",
      "|[0.6455365520110307,0.35446344798896934]|0.0       |\n",
      "|[0.4500210424472049,0.5499789575527951] |1.0       |\n",
      "|[0.6532507194634758,0.3467492805365242] |0.0       |\n",
      "|[0.6118755975910292,0.38812440240897084]|0.0       |\n",
      "|[0.6224629287211927,0.3775370712788073] |0.0       |\n",
      "+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under PR Curve: 0.9196399670688022\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTest)\n",
    "lrPred.select('probability','prediction').show(5,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\",\n",
    "                                          labelCol=\"diagnosisEncoded\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under PR Curve:\", aupr) #can see not super confident in predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|          352100.0|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "|          341300.0|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25|\n",
      "|          342200.0|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into dataframe\n",
    "df = spark.read.csv(DATA_FILEPATH2,  inferSchema=True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|             4.526|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|             3.585|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|             3.521|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "|             3.413|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25|\n",
      "|             3.422|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Scale the response variable median_house_value, dividing by 100000 \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"median_house_value\",col(\"median_house_value\")/100000)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|             4.526|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|  6.984126984126984|\n",
      "|             3.585|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|  6.238137082601054|\n",
      "|             3.521|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|  8.288135593220339|\n",
      "|             3.413|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25| 5.8173515981735155|\n",
      "|             3.422|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|  6.281853281853282|\n",
      "|             2.697|           4.0368|              52.0|      919.0|         213.0|     413.0|     193.0|   37.85|  -122.25|  4.761658031088083|\n",
      "|             2.992|           3.6591|              52.0|     2535.0|         489.0|    1094.0|     514.0|   37.84|  -122.25| 4.9319066147859925|\n",
      "|             2.414|             3.12|              52.0|     3104.0|         687.0|    1157.0|     647.0|   37.84|  -122.25|  4.797527047913447|\n",
      "|             2.267|           2.0804|              42.0|     2555.0|         665.0|    1206.0|     595.0|   37.84|  -122.26|  4.294117647058823|\n",
      "|             2.611|           3.6912|              52.0|     3549.0|         707.0|    1551.0|     714.0|   37.84|  -122.25|  4.970588235294118|\n",
      "|             2.815|           3.2031|              52.0|     2202.0|         434.0|     910.0|     402.0|   37.85|  -122.26|  5.477611940298507|\n",
      "|             2.418|           3.2705|              52.0|     3503.0|         752.0|    1504.0|     734.0|   37.85|  -122.26|  4.772479564032698|\n",
      "|             2.135|            3.075|              52.0|     2491.0|         474.0|    1098.0|     468.0|   37.85|  -122.26|  5.322649572649572|\n",
      "|             1.913|           2.6736|              52.0|      696.0|         191.0|     345.0|     174.0|   37.84|  -122.26|                4.0|\n",
      "|             1.592|           1.9167|              52.0|     2643.0|         626.0|    1212.0|     620.0|   37.85|  -122.26|  4.262903225806451|\n",
      "|               1.4|            2.125|              50.0|     1120.0|         283.0|     697.0|     264.0|   37.85|  -122.26|  4.242424242424242|\n",
      "|             1.525|            2.775|              52.0|     1966.0|         347.0|     793.0|     331.0|   37.85|  -122.27| 5.9395770392749245|\n",
      "|             1.555|           2.1202|              52.0|     1228.0|         293.0|     648.0|     303.0|   37.85|  -122.27|  4.052805280528053|\n",
      "|             1.587|           1.9911|              50.0|     2239.0|         455.0|     990.0|     419.0|   37.84|  -122.26|  5.343675417661098|\n",
      "|             1.629|           2.6033|              52.0|     1503.0|         298.0|     690.0|     275.0|   37.84|  -122.27|  5.465454545454546|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add new predictor: `rooms_per_household`\n",
    "\n",
    "df = df.withColumn(\"rooms_per_household\",col(\"total_rooms\")/col(\"households\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------------------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|features                                    |\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------------------------------+\n",
      "|4.526             |8.3252       |41.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |6.984126984126984  |[129.0,322.0,126.0,8.3252,6.984126984126984]|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feats,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(df)\n",
    "tr.select(\"*\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = tr.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|median_house_value|features                                      |scaledFeatures                                                                                       |\n",
      "+------------------+----------------------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "|0.14999           |[28.0,18.0,8.0,0.536,12.25]                   |[0.06684344246025051,0.016109651840083426,0.021007972177067596,0.28366233505166594,5.132877105254897]|\n",
      "|0.14999           |[73.0,85.0,38.0,1.6607,6.7105263157894735]    |[0.17427040355708168,0.07607335591150508,0.09978786784107108,0.8788769399632492,2.811780154328676]   |\n",
      "|0.14999           |[239.0,490.0,164.0,2.1,3.774390243902439]     |[0.570556526714281,0.4385405223133822,0.43066342962988574,1.1113636261352582,1.5815086750387162]     |\n",
      "|0.175             |[168.0,259.0,138.0,2.3667,3.572463768115942]  |[0.40106065476150304,0.23179999036564486,0.36238752005441605,1.2525068066544358,1.4968993865073434]  |\n",
      "|0.225             |[451.0,1230.0,375.0,1.0918,2.2533333333333334]|[1.0766568767704634,1.1008262090723675,0.9847486958000435,0.5778032414354642,0.9441700471162748]     |\n",
      "+------------------+----------------------------------------------+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|median_house_value|features                                              |scaledFeatures                                                                                      |\n",
      "+------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|0.14999           |[267.0,628.0,225.0,4.1932,3.568888888888889]          |[0.6202450325048916,0.5275213347043571,0.5794205235624388,2.1614338558378496,1.2774235360549526]    |\n",
      "|0.225             |[79.0,167.0,53.0,0.7916999999999998,2.018867924528302]|[0.18351819313815146,0.14028035492934338,0.13648572332804113,0.40809100058829184,0.7226196957288232]|\n",
      "|0.25              |[33.0,64.0,27.0,0.8570999999999998,1.6296296296296295]|[0.076659498399481,0.0537601360208262,0.06953046282749265,0.4418021935129783,0.5832984182899327]    |\n",
      "|0.342             |[153.0,112.0,47.0,1.0667,12.808510638297872]          |[0.35542131076123007,0.09408023803644586,0.1210345093663761,0.5498429586049399,4.584590179344567]   |\n",
      "|0.35              |[1747.0,6852.0,1597.0,2.3832,5.130244207889794]       |[4.058307384966463,5.755694562729706,4.112598116129843,1.228448241255548,1.8362843173042396]        |\n",
      "+------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In the training set, select all of these features and standardize them: (1 PT) - why only standardize the training features not the test features??\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol=\"scaledFeatures\") #withMean = True centers the data - not good if sparse data\n",
    "\n",
    "#Scale Training Data\n",
    "scalerTrain = scaler.fit(train)\n",
    "scaledTrain = scalerTrain.transform(train)\n",
    "\n",
    "scaledTrain.select(\"median_house_value\",\"features\",\"scaledFeatures\").show(5, truncate=False)\n",
    "\n",
    "#Scale Test Data\n",
    "scalerTest = scaler.fit(test)\n",
    "scaledTest = scalerTest.transform(test)\n",
    "\n",
    "scaledTest.select(\"median_house_value\",\"features\",\"scaledFeatures\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 10:39:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.081755424803359,-0.1614868306276797,0.10560698176227044,0.6345625835282451,-0.02760799353025227]\n",
      "Intercept: 0.7952155251919881\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaledFeatures',         # feature vector name\n",
    "                      labelCol='median_house_value',  # target variable name\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrain)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|median_house_value|     median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|        prediction|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|           0.14999|            4.1932|              52.0|      803.0|         267.0|     628.0|     225.0|   34.24|  -117.86|  3.568888888888889|[267.0,628.0,225....|[0.62024503250489...| 2.158224976508338|\n",
      "|             0.225|0.7916999999999998|              52.0|      107.0|          79.0|     167.0|      53.0|   37.95|  -121.29|  2.018867924528302|[79.0,167.0,53.0,...|[0.18351819313815...|1.0409887481720106|\n",
      "|              0.25|0.8570999999999998|              21.0|       44.0|          33.0|      64.0|      27.0|   32.79|  -114.65| 1.6296296296296295|[33.0,64.0,27.0,0...|[0.07665949839948...|1.0643916457541656|\n",
      "|             0.342|            1.0667|              31.0|      602.0|         153.0|     112.0|      47.0|    41.5|  -121.76| 12.808510638297872|[153.0,112.0,47.0...|[0.35542131076123...|1.0442009475324578|\n",
      "|              0.35|            2.3832|              39.0|     8193.0|        1747.0|    6852.0|    1597.0|   34.59|  -117.37|  5.130244207889794|[1747.0,6852.0,15...|[4.05830738496646...|1.3606855348320237|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------\n",
      "METRICS\n",
      "Mean Squared Error: 0.6851990610397196\n",
      "R Squared: 0.48710687489849125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTest)\n",
    "lrPred.show(5)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_house_value\")\n",
    "\n",
    "print('-'*20)\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
