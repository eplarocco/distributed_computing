{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Assignment: Commercial Data Analysis\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS  \n",
    "In this assignment, you will work with a dataset containing information about businesses.  \n",
    "Each record is a business location.  Follow the steps below, writing and running the code in blocks, and displaying the solutions.  \n",
    "\n",
    "Each question part is worth 1 POINT, for a total of 15 POINTS.\n",
    "\n",
    "Hint: reaching deeper fields in json hierarchy can be done like this:  \n",
    "\n",
    "`df.select('address.street_number')`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/06 15:46:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that read.json can read a zipped JSON directly - has to be done on rivanna - /standard/ds7200-apt4c/large_datasets/part-00000-a159c41a-bc58-4476-9b78-c437667f9c2b-c000.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (1 PT) Read in the dataset and show the number of records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/standard/ds7200-apt4c/large_datasets/part-00000-a159c41a-bc58-4476-9b78-c437667f9c2b-c000.json.gz\"\n",
    "df = spark.read.json(data)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#df.select('address.coordinates.lat', 'reviews.element.content').show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (1 PT) Show the first 5 records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (1 PT) Show the first 5 street addresses which are not null**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (1 PT) Location**  \n",
    "\n",
    "Count the number of records where the city is Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (1 PT) Hours**  \n",
    "\n",
    "Count the number of records where closing time on Thursday is 8pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. (1 PT) Location and Hours**  \n",
    "\n",
    "Count the number of records where city is Phoenix and closing time on Thursday is 8pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. (1 PT) Price Range**  \n",
    "\n",
    "Price range is quoted in number of dollar signs.  Count the number of records with price range greater than or equal to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. (1 PT) COMPANY HEADQUARTERS**  \n",
    "\n",
    "For the `address.is_headquarters` field:  \n",
    "how many locations are HQ / are NOT HQ / are null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. (1 PT) Webpage URLs**  \n",
    "\n",
    "Register the dataframe as a temp table.  \n",
    "Next, use Spark SQL to select only the webpage title column, filtering on rows where the webpage url (accessed under `webpage.url`) is *Target.com*. \n",
    "\n",
    "Show only one resulting row and don't truncate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. (1 PT) Analysis on Ratings**  \n",
    "\n",
    "The reviews contains information such as the number of stars for each review (the *rating*).  \n",
    "The ratings are stored in an array (`reviews.stars`) for each business location (you should check for yourself). Return the top five most common rating arrays.  For example, an array might look like: \n",
    "[5, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. More work with Ratings**  \n",
    "\n",
    "For this question, you will filter out null ratings and then compute the average rating for each business location (using the field: `id`).\n",
    "\n",
    "\n",
    "a) (1 PT) Create a new dataframe retaining two fields: `id`, `reviews.stars`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (1 PT) Create a row for each rating  \n",
    "hint: use the `withColumn()` and `explode()` functions  \n",
    "you will need to import the `explode()` function by issuing:\n",
    "\n",
    "`from pyspark.sql.functions import explode`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (1 PT) Return a count of the number of ratings in this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) (1 PT) Drop rows where the rating is null, and return a count of the number of non-null ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) (1 PT) Compute the average rating, grouped by `id`. After the average is computed, sort by `id` in ascending order and show the top 10 records.  \n",
    " \n",
    "hint:   \n",
    "this can all be done in one line using the `agg()` function  \n",
    "this `id` should be at the top: 000136e65d50c3b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
