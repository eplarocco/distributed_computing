{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Training\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: June 7, 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Review the source papers and add relevant details\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SOURCES: \n",
    "\n",
    "- [Distributed Machine Learning Frameworks and its Benefits](https://www.xenonstack.com/blog/distributed-ml-framework#:~:text=In%20distributed%20machine%20learning%2C%20model,and%20training%20each%20split%20separately.)\n",
    "\n",
    "- [Distributed Training with Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training?view=azureml-api-2/)\n",
    "\n",
    "- [Distributed Training: Guide for Data Scientists\n",
    "](https://neptune.ai/blog/distributed-training)\n",
    "\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin.\n",
    "\n",
    "- [Distributed model training II: Parameter Server and AllReduce](http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/)\n",
    "\n",
    "Need to Review:\n",
    "\n",
    "- [Distributed Machine Learning and the Parameter Server](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture22.pdf)\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "\n",
    "- Explain approaches for distributed model training\n",
    "- Identify the benefits and challenges of synchronous and asynchronous training\n",
    "- Explain the parameter server and Allreduce algorithms\n",
    "- Explain why Ring-Allreduce can work better than Allreduce \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Data parallelism and model parallelism\n",
    "- Asynchronous training vs Synchronous training \n",
    "- Parameter server algorithm\n",
    "- Allreduce algorithm and Ring-Allreduce algorithm\n",
    "- Vertical partitioning and horizontal partitioning of a model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Why use Distributed Model Training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For massive training sets, it may not be possible to train a model on a single machine. \n",
    "\n",
    "This may be the case for deep learning models.\n",
    "\n",
    "In *distributed training*, the workload to train a model is split up and shared among worker nodes. \n",
    "\n",
    "The concepts, benefits, and challenges are similar to what we've learned earlier.\n",
    "\n",
    "The work can be parallelized to speed up training.\n",
    "\n",
    "This process introduces benefits but also complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Parallelism and Model Parallelism\n",
    "\n",
    "Two main types of distributed training: *data parallelism* and *model parallelism*.\n",
    "\n",
    "#### Data Parallelism\n",
    "\n",
    "This follows the approach used by Spark\n",
    "\n",
    "Data is divided into partitions\n",
    "\n",
    "Number of partitions = total number of available nodes\n",
    "\n",
    "Model is copied in each worker node\n",
    "\n",
    "Each node operates on its subset of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data_parallelism.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node must:\n",
    "\n",
    "- Independently compute errors between training sample predictions and labels\n",
    "- Update its model based on errors\n",
    "- Communicate all of its changes to the other nodes to update their corresponding models\n",
    "\n",
    "Worker nodes need to synchronize gradients at end of batch computation to ensure they're training a consistent model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parallelism\n",
    "\n",
    "In some cases, the model may be too large for a worker\n",
    "\n",
    "The strategy is to segment the model into different parts that run concurrently in different workers\n",
    "\n",
    "Each model part runs on same data\n",
    "\n",
    "Scalability depends on degree of task parallelization of algorithm\n",
    "\n",
    "Worker nodes need to synchronize shared parameters, usually once for each forward or backward-propagation step \n",
    "\n",
    "More complex to implement than data parallelism\n",
    "\n",
    "**Example of model parallelized on two GPUs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./model_parallelism.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model may be divided horizontally or vertically into different parts \n",
    "\n",
    "The parts run concurrently in different workers with each worker running on the same data\n",
    "\n",
    "Worker needs to synchronize shared parameters, usually once for each forward or backward-propagation step\n",
    "\n",
    "This **illustration** shows different partitions. Vertical partitioning keeps together all neurons in a layer, which works well for training deep learning models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./horiz_vert_part.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### III. Synchronous Training\n",
    "\n",
    "Consider data parallelism case: \n",
    "- Data is divided into partitions\n",
    "- Each partition is sent to a worker \n",
    "- Each worker has full replica of model. Training is done on its partition. \n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "In synchronous training, forward pass begins at same time for each worker\n",
    "\n",
    "Each worker computes different output and gradients\n",
    "\n",
    "Each worker waits for the others to complete training loops and calculate respective gradients\n",
    "\n",
    "After all workers have computed gradients, they communicate with each other and aggregate gradients using *Allreduce algorithm* (below)\n",
    "\n",
    "After all gradients are combined, the updated gradients are copied to all workers. \n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "Each worker performs backward pass and updates their local weights\n",
    "\n",
    "Each worker will have different gradients as they are trained on different subsets of data\n",
    "\n",
    "However, at any point in time, all workers have the same weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Synchronous Algorithm Example: Allreduce Algorithm\n",
    "\n",
    "Each node has a subset of the data \n",
    "\n",
    "Each node calculates the model's gradient and distributes it to the other nodes\n",
    "\n",
    "Algorithm reduces the target arrays in all workers to a single array and returns the resultant array to all workers\n",
    "\n",
    "Gradients are combined which updates model weights on each node\n",
    "\n",
    "Notice from this example that the operation proceeds elementwise. The reduction is general (e.g., it might add the values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"allreduce.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are different implementations:\n",
    "\n",
    "**1. Naive Allreduce**\n",
    "\n",
    "Each worker sends gradients to single worker called the *driver*.  \n",
    "\n",
    "The driver reduces the gradients and sends updated gradients to all workers.  \n",
    "\n",
    "**Challenge with this approach** is the driver is a bottleneck. As the number of nodes increases, communcation cost and reduction step scales poorly.\n",
    "\n",
    "--\n",
    "\n",
    "**2. Ring-Allreduce** \n",
    "  \n",
    "Workers are set up in a ring (recall *consistent hashing*)  \n",
    "\n",
    "Each worker is in charge of subset of parameters shared only with the next worker in the ring  \n",
    "\n",
    "Less time spent sending data and more time spent doing computations on data locally on each GPU  \n",
    "\n",
    "This allows for efficient averaging of gradients in neural networks across many devices and many nodes  \n",
    "\n",
    "Retains the determinism and predictable convergence properties of synchronous stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ringreduce.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### IV. Asynchronous Training\n",
    "\n",
    "We apply data parallelism here as well\n",
    "\n",
    "Asynchronous training can be more efficient than synchronous training since there is no waiting. \n",
    "\n",
    "This is especially helpful when there is variation in the computing power across workers.\n",
    "\n",
    "Thus in asynchronous training, we want workers to work independently in such a way that a worker need not wait for any other worker in the cluster. One way to achieve this is by using a parameter server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous Algorithm Example: Parameter Server\n",
    "\n",
    "In this approach, weights and biases of ML model are distributed across nodes in cluster  \n",
    "\n",
    "A copy of the model is stored on each node and a centralized *parameter server* manages model changes.  \n",
    "This pattern is called *centralized training*.\n",
    "\n",
    "Here is how it works:\n",
    "\n",
    "1. Designate some nodes as parameter servers, while others train the model\n",
    "\n",
    "- Parameter servers store model parameters, update global state of model\n",
    "- Training workers run training loop, calculate gradients and loss. They each use subset of data.\n",
    "\n",
    "2. Each training worker fetches parameters from parameter servers\n",
    "3. Each training worker performs training loop, sends gradients back to all parameter servers, which updates model parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./parameter_server.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges with this approach**\n",
    "\n",
    "At any given time, only one worker is using updated version of model. Others are using stale version. \n",
    "\n",
    "If one worker is used as parameter server, it can be a bottleneck and single point of failure.  \n",
    "Using multiple parameter servers alleviates this issue but adds communication costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Server Example: Gorila - General Reinforcement Learning Architecture**\n",
    "\n",
    "*Source: Mastering Reinforcement Learning with Python, Enes Bilgin.*\n",
    "\n",
    "Background:  \n",
    "- Reinforcement learning (RL) requires massive training datasets. There is no label used; instead, a reward function is specified.\n",
    "- An agent exists in some state (e.g., an investor holds a portfolio of stocks) and must take a series of actions over time (buy/sell orders).\n",
    "- Everything outside of the agent is the environment (the economy, the stock markets)\n",
    "- We might have historical data (experiences) that can be used\n",
    "- The goal is to learn the best action to take given each state. This is the *optimal policy*. It needs to maximize long-term reward.\n",
    "- In Deep RL, a common approach is to use a neural network for learning the optimal policy. The *Q-network* is one approach.\n",
    "\n",
    "\n",
    "Strategy: we need to simulate experiences, and we can parallelize this. We store experiences in a replay buffer.\n",
    "\n",
    "Here are the important components:\n",
    "\n",
    "**Actors:**  \n",
    "Processes that interact with copy of environment given a policy, take the action, observe the reward, and transition to the next state.\n",
    "\n",
    "Actors get a copy of the Q-network from a parameter server\n",
    "\n",
    "**Replay Buffer:**  \n",
    "Actors collect experiences and store them in a replay buffer. This can be done in a distributed fashion.\n",
    "\n",
    "**Learners:**  \n",
    "Learners calculate the gradients that update the Q-network in the parameter server.  \n",
    "A learner has a copy of the Q-network, it samples experiences from the buffer, calculates the loss and gradients, and sends them back to the parameter server\n",
    "\n",
    "**Parameter Server:**  \n",
    "The parameter server stores the main copy of the Q-network. As learning progresses, updates happen here.    \n",
    "All processes sync their version of the Q-network from here.\n",
    "\n",
    "--\n",
    "\n",
    "**Next, we provide an overview of Gorila**\n",
    "\n",
    "Gorila provides a general framework to parallelize deep Q-learning\n",
    "\n",
    "It consists of bundles which can be distributed to workers, where each bundle comprises an action, learner, and local replay buffer.  \n",
    "\n",
    "Next we show an illustration of the approach. For our purposes, the details are unimportant; the takeaway is how the parameter server supports learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./gorila.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the learners send gradients back to the parameter server\n",
    "\n",
    "The learners get the synced Q-network from the parameter server\n",
    "\n",
    "**Shortcoming:** There is a lot of passing around of parameters between the actors, learners, and parameter server.  \n",
    "This can be a significant communication load.\n",
    "\n",
    "If you are interested, you can [explore](https://arxiv.org/pdf/1803.00933) how the *Ape-X* architecture improves Gorila!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### V. Implementation\n",
    "\n",
    "**Elephas** is a Keras add-on that allows you to use Spark to execute distributed deep learning models at scale.\n",
    "\n",
    "**Amazon SageMaker** offers \n",
    "\n",
    "**Horovod**. Open-source distributed training framework developed at Uber for TensorFlow, Keras, PyTorch, and MXNet.  \n",
    "Supports practical distributed training over several GPUs and nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Conclusions\n",
    "\n",
    "We have studied different approaches for distributed model training\n",
    "\n",
    "Benefits can include scalability, efficiency, and fault tolerance \n",
    "\n",
    "These come at the cost of increased network communication load\n",
    "\n",
    "Data parallelism (partitioning the data across nodes) is easier than model parallelism\n",
    "\n",
    "Model parallelism is useful when the model won't fit on a single machine. \n",
    "\n",
    "Vertical partitioning is easier for deep learning, as layers are kept together on workers\n",
    "\n",
    "There are several packages that implement distributed model training\n",
    "\n",
    "**Tradeoffs**\n",
    "\n",
    "As always, there are tradeoffs to the methods:\n",
    "\n",
    "- Centralized training, such as the parameter server approach, can introduce a communication bottleneck. More parameter servers can be added to reduce the bottleneck.\n",
    "\n",
    "- The parameter server approach is asynchronous which can increase efficiency: there is no waiting for workers. This is especially helpful when the nodes have differences in processing power.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
